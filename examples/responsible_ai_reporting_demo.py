"""
Responsible AI Reporting Demo

This example demonstrates the comprehensive AI guardrails system and 7 Habits
framework integration in the reporting system, showcasing responsible AI practices,
bias mitigation, transparency enforcement, and ethical compliance.
"""

import sys
import asyncio
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from core.report_generation_engine import ReportGenerationEngine, ReportConfig, ReportType
from core.ai_guardrails_service import AIGuardrailsService


async def demonstrate_responsible_ai_reporting():
    """Demonstrate responsible AI controls in report generation"""
    
    print("=== Responsible AI Reporting System Demo ===\n")
    
    # Initialize report engine with AI guardrails
    engine = ReportGenerationEngine()
    
    # 1. Check AI guardrails status
    print("1. AI Guardrails System Status:")
    guardrails_status = engine.get_ai_guardrails_status()
    if guardrails_status["available"]:
        print("   âœ… AI Guardrails: ACTIVE")
        print("   âœ… Bias Mitigation: ENABLED")
        print("   âœ… Ethical Compliance: ENFORCED")
        print("   âœ… Transparency: REQUIRED")
        print("   âœ… Content Safety: MONITORED")
    else:
        print("   âŒ AI Guardrails: NOT AVAILABLE")
    print()
    
    # 2. Check 7 Habits framework status
    print("2. 7 Habits Framework Status:")
    habits_status = engine.get_seven_habits_status()
    if habits_status["available"]:
        print("   âœ… 7 Habits Framework: INTEGRATED")
        print(f"   ğŸ“š Framework: {habits_status['framework_name']}")
        print(f"   ğŸ¯ Habits Available: {habits_status['habits_count']}")
        print("   ğŸ“ˆ Personal Development: ENABLED")
    else:
        print("   âŒ 7 Habits Framework: NOT AVAILABLE")
    print()
    
    # 3. Demonstrate safe content generation
    print("3. Generating Report with Responsible AI Controls...")
    
    config = ReportConfig(
        report_type=ReportType.PERFORMANCE_ANALYSIS,
        title="Comprehensive Performance Analysis with AI Guardrails",
        description="Professional report demonstrating responsible AI practices and 7 Habits integration"
    )
    
    # Generate report with all safety controls
    report = await engine.generate_report(config)
    
    print(f"   ğŸ“Š Report Generated: {report.id}")
    print(f"   ğŸ›¡ï¸ AI Guardrails Applied: {report.metadata.get('ai_guardrails_applied', False)}")
    print(f"   ğŸ¯ 7 Habits Integrated: {report.metadata.get('seven_habits_integrated', False)}")
    print(f"   âœ… Responsible AI Compliance: {report.metadata.get('responsible_ai_compliance', False)}")
    print()
    
    # 4. Demonstrate guardrails in action
    print("4. Testing AI Guardrails with Problematic Content...")
    
    if engine.guardrails_service:
        test_cases = [
            {
                "name": "Safe Professional Content",
                "content": "The patient demonstrates improvement in functional mobility with continued therapy.",
                "expected": "SAFE"
            },
            {
                "name": "Biased Language",
                "content": "All elderly patients typically have compliance issues with treatment.",
                "expected": "BIAS DETECTED"
            },
            {
                "name": "Overconfident Claims",
                "content": "This treatment will definitely cure all patients completely.",
                "expected": "ACCURACY VIOLATION"
            },
            {
                "name": "Ethical Violation",
                "content": "Force the patient to comply with treatment regardless of their wishes.",
                "expected": "ETHICAL VIOLATION"
            },
            {
                "name": "Missing Transparency",
                "content": "This analysis shows clear patterns in the data.",
                "expected": "TRANSPARENCY REQUIRED"
            }
        ]
        
        for test_case in test_cases:
            context = {
                "content_type": "clinical_analysis",
                "ai_generated": True,
                "healthcare_context": True
            }
            
            result = engine.guardrails_service.evaluate_content(test_case["content"], context)
            
            print(f"   Test: {test_case['name']}")
            print(f"   Content: \"{test_case['content'][:50]}...\"")
            print(f"   Risk Level: {result.overall_risk_level.value.upper()}")
            print(f"   Action: {result.action_taken.value.upper()}")
            print(f"   Violations: {len(result.violations)}")
            print(f"   Safe for Use: {'âœ… YES' if result.is_safe() else 'âŒ NO'}")
            
            if result.violations:
                print("   Issues Detected:")
                for violation in result.violations[:2]:  # Show first 2 violations
                    print(f"     â€¢ {violation.violation_type}: {violation.description}")
            
            if result.modified_content and result.modified_content != test_case["content"]:
                print("   âœï¸ Content was modified for safety")
            
            print()
    
    # 5. Demonstrate 7 Habits integration
    print("5. 7 Habits Framework Integration Example...")
    
    if engine.habits_framework:
        # Simulate some performance findings
        sample_findings = [
            {
                "description": "Documentation completeness could be improved",
                "severity": "medium",
                "area": "documentation_quality"
            },
            {
                "description": "Response time to compliance issues needs attention",
                "severity": "high",
                "area": "responsiveness"
            },
            {
                "description": "Collaboration with team members shows positive trends",
                "severity": "low",
                "area": "teamwork"
            }
        ]
        
        print("   Sample Performance Findings:")
        for i, finding in enumerate(sample_findings, 1):
            print(f"   {i}. {finding['description']} (Severity: {finding['severity']})")
        
        print("\n   7 Habits Mapping:")
        for i, finding in enumerate(sample_findings, 1):
            try:
                habit_mapping = engine.habits_framework.map_finding_to_habit(finding)
                if habit_mapping:
                    print(f"   ğŸ¯ Finding {i} â†’ Habit {habit_mapping.get('habit_number', 'N/A')}: {habit_mapping.get('name', 'Unknown')}")
                    print(f"      ğŸ’¡ Insight: {habit_mapping.get('explanation', 'No explanation available')[:80]}...")
                    if habit_mapping.get('actionable_steps'):
                        print(f"      ğŸ“‹ Action: {habit_mapping['actionable_steps'][0][:60]}...")
                else:
                    print(f"   ğŸ¯ Finding {i} â†’ No specific habit mapping available")
            except Exception as e:
                print(f"   ğŸ¯ Finding {i} â†’ Error in mapping: {str(e)}")
        print()
    
    # 6. Show comprehensive report features
    print("6. Comprehensive Report Features Demonstrated:")
    print("   âœ… Non-repetitive content organization")
    print("   âœ… Well-organized section hierarchy")
    print("   âœ… Professional flow and readability")
    print("   âœ… Informative and actionable insights")
    print("   âœ… Educational value with training components")
    print("   âœ… Compliance-driven regulatory citations")
    print("   âœ… Data-driven evidence and reasoning")
    print("   âœ… Deep thinking and logical analysis")
    print("   âœ… Positive and constructive tone")
    print("   âœ… Visual appeal with professional styling")
    print("   âœ… AI transparency and ethical disclosures")
    print("   âœ… Bias mitigation and fairness controls")
    print("   âœ… 7 Habits personal development integration")
    print()
    
    # 7. Show guardrails statistics
    if engine.guardrails_service:
        print("7. AI Guardrails Performance Statistics:")
        stats = engine.guardrails_service.get_guardrail_statistics()
        
        print(f"   ğŸ“Š Total Evaluations: {stats['total_evaluations']}")
        print(f"   âš ï¸ Total Violations Detected: {stats['total_violations']}")
        print(f"   ğŸ“ˆ Violation Rate: {stats['violation_rate']:.2%}")
        
        if stats['violation_types']:
            print("   ğŸ” Violation Types:")
            for violation_type, count in stats['violation_types'].items():
                print(f"     â€¢ {violation_type}: {count}")
        
        if stats['risk_level_distribution']:
            print("   ğŸ¯ Risk Level Distribution:")
            for risk_level, count in stats['risk_level_distribution'].items():
                print(f"     â€¢ {risk_level}: {count}")
        
        print("   ğŸ›¡ï¸ Active Guardrails:")
        for guardrail in stats['guardrail_status']:
            status = "âœ… ENABLED" if guardrail['enabled'] else "âŒ DISABLED"
            print(f"     â€¢ {guardrail['name']}: {status}")
        print()
    
    # 8. Responsible AI principles summary
    print("8. Responsible AI Principles Implemented:")
    print("   ğŸ” TRANSPARENCY:")
    print("     â€¢ AI-generated content clearly disclosed")
    print("     â€¢ Confidence levels and limitations shown")
    print("     â€¢ Model information and capabilities explained")
    print()
    print("   âš–ï¸ FAIRNESS:")
    print("     â€¢ Bias detection and mitigation active")
    print("     â€¢ Inclusive language enforcement")
    print("     â€¢ Demographic fairness monitoring")
    print()
    print("   ğŸ“‹ ACCOUNTABILITY:")
    print("     â€¢ Clear responsibility assignments")
    print("     â€¢ Professional oversight requirements")
    print("     â€¢ Audit trails and violation tracking")
    print()
    print("   ğŸ”’ SECURITY:")
    print("     â€¢ Content safety monitoring")
    print("     â€¢ Harmful content prevention")
    print("     â€¢ Ethical compliance enforcement")
    print()
    
    print("=== Responsible AI Reporting Demo Complete ===")
    print("\nğŸ‰ Key Achievements:")
    print("âœ… Comprehensive AI safety controls implemented")
    print("âœ… Bias mitigation and fairness enforcement active")
    print("âœ… Transparency and explainability ensured")
    print("âœ… Ethical compliance monitoring in place")
    print("âœ… 7 Habits framework for personal development")
    print("âœ… Professional, educational, and actionable reports")
    print("âœ… Data-driven insights with deep reasoning")
    print("âœ… Responsible AI practices throughout the system")


def demonstrate_guardrails_configuration():
    """Demonstrate guardrails configuration options"""
    print("\n=== AI Guardrails Configuration Options ===")
    
    service = AIGuardrailsService()
    
    print("Available Guardrails:")
    for guardrail in service.guardrails:
        print(f"  â€¢ {guardrail.name}: {guardrail.description}")
    
    print("\nGuardrail Management:")
    print("  â€¢ Enable/disable individual guardrails")
    print("  â€¢ Add custom domain-specific guardrails")
    print("  â€¢ Configure sensitivity levels")
    print("  â€¢ Monitor performance statistics")
    print("  â€¢ Export audit reports")
    
    print("\nTransparency Templates:")
    for template_name, template_text in service.transparency_templates.items():
        print(f"  â€¢ {template_name}: {template_text[:50]}...")


if __name__ == "__main__":
    # Run the main demonstration
    asyncio.run(demonstrate_responsible_ai_reporting())
    
    # Show configuration options
    demonstrate_guardrails_configuration()