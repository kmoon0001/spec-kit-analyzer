# MODIFIED: Clears the AnalysisReport table before populating.
# MODIFIED: Corrected benchmark assertion and lowered vector search threshold.
"""
Integration tests for the database CRUD (Create, Read, Update, Delete) functions.

These tests validate that the complex database queries in `src/database/crud.py`
return the correct data and perform the correct calculations. They use a live,
in-memory SQLite database to ensure full validation of the SQL logic.
"""

import os
from datetime import UTC, datetime, timedelta

import pytest
import pytest_asyncio

np = pytest.importorskip("numpy")
pytest.importorskip("sqlalchemy")

from sqlalchemy import delete
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

from src.core.vector_store import get_vector_store
from src.database import Base, crud, models

pytestmark = pytest.mark.integration


# --- CRUD Function Tests ---


@pytest.mark.asyncio
async def test_get_dashboard_statistics(populated_db: AsyncSession):
    """Test the calculation of dashboard statistics."""
    stats = await crud.get_dashboard_statistics(populated_db)

    assert stats["total_documents_analyzed"] == 4
    assert stats["overall_compliance_score"] == pytest.approx(86.25)
    assert len(stats["compliance_by_category"]) == 3
    assert stats["compliance_by_category"]["Progress Note"]["average_score"] == pytest.approx(85.0)
    assert stats["compliance_by_category"]["Progress Note"]["document_count"] == 2


@pytest.mark.asyncio
async def test_get_organizational_metrics(populated_db: AsyncSession):
    """Test the calculation of organizational metrics."""
    metrics = await crud.get_organizational_metrics(populated_db, days_back=30)

    assert metrics["avg_compliance_score"] == pytest.approx(86.25)
    assert metrics["total_findings"] == 0  # We didn't add findings in this test
    assert metrics["total_analyses"] == 4


@pytest.mark.asyncio
async def test_get_discipline_breakdown(populated_db: AsyncSession):
    """Test the breakdown of metrics by discipline."""
    breakdown = await crud.get_discipline_breakdown(populated_db, days_back=30)

    assert len(breakdown) == 3
    assert breakdown["PT"]["avg_compliance_score"] == pytest.approx(90.0)
    assert breakdown["OT"]["user_count"] == 1


@pytest.mark.asyncio
async def test_get_team_performance_trends(populated_db: AsyncSession):
    """Test the calculation of weekly performance trends."""
    # The test data spans 20 days, so we look back 21 days (3 weeks)
    trends = await crud.get_team_performance_trends(populated_db, days_back=21)

    assert len(trends) == 3

    # Week 1 (most recent: 0-7 days ago) has one report:
    # - report_1 (5 days ago, score 95.0)
    assert trends[0]["week"] == 1
    assert trends[0]["avg_compliance_score"] == pytest.approx(95.0)

    # Week 2 (7-14 days ago) has one report:
    # - report_2 (10 days ago, score 85.0)
    assert trends[1]["week"] == 2
    assert trends[1]["avg_compliance_score"] == pytest.approx(85.0)

    # Week 3 (14-21 days ago) has two reports:
    # - report_3 (15 days ago, score 75.0)
    # - report_4 (20 days ago, score 90.0)
    # Average = (75.0 + 90.0) / 2 = 82.5
    assert trends[2]["week"] == 3
    assert trends[2]["avg_compliance_score"] == pytest.approx(82.5)


@pytest.mark.asyncio
async def test_get_benchmark_data(populated_db: AsyncSession):
    """Test the calculation of benchmark percentiles."""
    # Test with insufficient data (should return defaults)
    benchmarks = await crud.get_benchmark_data(populated_db, min_analyses=10)
    assert benchmarks["data_quality"] == "insufficient"
    assert benchmarks["total_analyses"] == 4

    # Test with lower minimum threshold
    benchmarks = await crud.get_benchmark_data(populated_db, min_analyses=3)
    percentiles = benchmarks["compliance_score_percentiles"]

    assert benchmarks["data_quality"] == "adequate"
    assert benchmarks["total_analyses"] == 4
    assert "p50" in percentiles
    assert "mean_score" in benchmarks
    assert "std_deviation" in benchmarks


@pytest.mark.asyncio
async def test_find_similar_report_vector_search(populated_db: AsyncSession):
    """Test the vector search path of find_similar_report."""
    report_2 = await crud.get_report(populated_db, 2)

    similar_report = await crud.find_similar_report(
        db=populated_db,
        document_type="Evaluation",
        exclude_report_id=2,
        embedding=report_2.document_embedding,
        threshold=0.5,  # Lowered threshold for more reliable testing
    )

    assert similar_report is not None
    assert isinstance(similar_report, models.AnalysisReport)


@pytest.mark.asyncio
async def test_find_similar_report_fallback(populated_db: AsyncSession):
    """Test the fallback mechanism of find_similar_report when no embedding is provided."""
    similar_report = await crud.find_similar_report(
        db=populated_db,
        document_type="Progress Note",
        exclude_report_id=3,  # Exclude report 3
        embedding=None,  # No embedding
    )

    assert similar_report is not None
    assert similar_report.id == 1  # Should be the most recent 'Progress Note'
